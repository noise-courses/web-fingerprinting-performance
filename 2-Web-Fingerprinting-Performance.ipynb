{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ad598f",
   "metadata": {},
   "source": [
    "# Website Fingerprinting and Performance\n",
    "\n",
    "In this assignment, you will perform a classification and a regression task on a packet trace of web downloads that you capture.  There are three steps to the assignment:\n",
    "\n",
    "1. Capturing a web packet trace using Wireshark\n",
    "2. Performing a classification task to \"fingerprint\" websites.\n",
    "3. Performing a regression task to predict web page load time (PLT).\n",
    "\n",
    "All of these are real tasks performed by network operators, so you will be developing skills that are used to run real networks!\n",
    "\n",
    "**Important instructions, note well**: \n",
    "\n",
    "* Your notebook should be based on this published template, and it should run to completion \n",
    "upon running \"Restart and Run All\" from the Kernel menu. \n",
    "* Because Github limits filesize to < 50 MB, you should take care in capturing your packet trace. If your trace is larger than 50 MB or you won't be able to upload it. I tested this process and was able to geenrate traces < 50 MB. We prefer this, to make grading easier. If your trace is larger than 50 MB, you can typically make it smaller by saving just the headers of your packet capture. I was able to do this with wireshark, but if you run into difficulty, you can use `tcpdump` with the `-s` option to limit the number of bytes captured for each packet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4ecc94",
   "metadata": {},
   "source": [
    "## Part 1: Packet Capture\n",
    "\n",
    "1. Use wireshark to capture *two* packet traces of web page downloads. Visit two websites **10 times**:\n",
    "\n",
    "* The [UChicago CS website](https://www.cs.uchicago.edu/).\n",
    "* **One** of the following three sites: [Amazon](https://amazon.com/), [New York Times](https://nytimes.com), [Netflix](https://netflix.com)\n",
    "\n",
    "**Hint:** Because your browser caches objects, you may need to clear your cache, or at least use shift+reload, to force a reload of the webpage each time.\n",
    "\n",
    "**Important:** Close all other browser tabs while you perform this experiment, and wait at least five seconds between page loads so that the two page loads do not overlap in your trace. Restart your wireshark capture to create two separate packet captures (or, optionally, post-process your traffic so that you have separate traces for each web download).\n",
    "\n",
    "If you capture this on your laptop, there may be other traffic in your trace that you need to filter, so to keep things as simple as possible, try to avoid having other applications running on your laptop at the same time.\n",
    "\n",
    "2. Save your two packet captures as `uchicago-cs.pcap` and `netflix.pcap`, `amazon.pcap`, or `nytimes.pcap` in the `data/` directory of your git repository, where we have included also an `example-uchicago-cs.pcap`.  \n",
    "\n",
    "3. Use the `netml` library (version 0.3.0!) to sanity check your packet trace.  \n",
    "\n",
    "In the cell below, **plot a histogram** of the total number of bytes to each destination, sorted by destination IP address. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31aca2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d51e0030",
   "metadata": {},
   "source": [
    "## Part 2: Website Fingerprinting\n",
    "\n",
    "Most web traffic is encrypted, making it more difficult to determine the website that you are visiting. Even if DNS traffic is encrypted, however (yours most likely is not), various properties of the web traffic trace can reveal the websites you are visiting. This process is called *website fingerprinting*, and there are many research papers on the topic. [This paper](https://nymity.ch/tor-dns/pdf/Panchenko2016a.pdf) is a particularly good read.\n",
    "\n",
    "Your task in this part of the assignment is to use a classifier to perform a website fingerprinting attack on the two websites that you visited in Part 1 of the assignment. Because there are only two sites, chosen to be slightly different in nature (academic vs. commercial/advertising-heavy), the traffic patterns will likely look different enough in nature, across certain dimensions. You should try to identify one or more of those dimensions and implement a classifier that distinguishes your two websites.\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "As we've discussed in class, application of domain knowledge is important for designing an effective classifier. Identify some features of the traffic flows where you think these two websites are likely to exhibit different characteristics.  \n",
    "\n",
    "Plot the differences in distributions using a box and whisker plot, a probability distribution function (PDF), or a cumulative distribution function (CDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe967a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4cb189e",
   "metadata": {},
   "source": [
    "### Train the Classifier\n",
    "\n",
    "Take the features above and use them to train a classifier based on your two websites. This is a simple two-class classifier, and so just about any model we have discussed in class to-date (e.g., logistic regression, SVM) should work.\n",
    "\n",
    "**Consider the entire trace that you captured in Part 1 as your training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe7924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32f27a61",
   "metadata": {},
   "source": [
    "### Test the Classifier\n",
    "\n",
    "Since you used the entire trace above to train your model, you will need additional data to test your model.  Repeat your packet capture from Step 1 (10 downloads of each site), but hold use the new capture for your test set.  \n",
    "\n",
    "1. Save your new packet capture files as `uchicago-cs-test.pcap` and `netflix-test.pcap`, `amazon-test.pcap`, or `nytimes-test.pcap` in the `data/` directory of your git repository. \n",
    "\n",
    "2. Evaluate your classifier above using the test data that you have just captured on the two sites on which you have trained your model, using (at least) the following techniques:\n",
    "* Confusion matrix\n",
    "* ROC/AUC\n",
    "\n",
    "The evaluation will be a little crude because you only have 10 data points each in your test and training sets. For an **optional** activitiy, feel free to repeat this exersise with more data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96571a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5f380d2",
   "metadata": {},
   "source": [
    "## Part 3: Page Load Time Estimation\n",
    "\n",
    "Another task that network operators face is application performance estimation. One instance of application performance estimation is *web page load time estimation*. Specifically, an operator may want to know how long it took you to download a website, *and* what might happen if network conditions changed.\n",
    "\n",
    "In this part of the assignment, you will build a simple regression model between network characteristics---throughput and latency---and page load time. Once you have built that model, you can use it to make predictions about how changes to performance would affect page load time.\n",
    "\n",
    "Your task is to train a linear regression model (optionally with polynomial expansion) to relate the features of 1. throughput to the web server and 2. round-trip latency to the web server to how long it took to load the web page.\n",
    "\n",
    "You will use the same traces as in Part 1 (and 2) of this assignment of your training and test set.\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "For each page download instance, use the `netml` STATS option to generate statistics for throughput (bytes per second) and latency.  \n",
    "\n",
    "**Important**: `netml` does not capture round-trip latency or web page load time, so you will have to compute those separately, for each of your downloads!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9194784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1425b36a",
   "metadata": {},
   "source": [
    "### Train a Classifier\n",
    "\n",
    "Train a linear regression model (optionally with polynomial expansion) to use the features of 1. throughput to the web server and 2. round-trip latency to the web server to predict page load time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f7e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6279f700",
   "metadata": {},
   "source": [
    "### Test the Classifier\n",
    "\n",
    "Use your test packet captures from Part 2 to test your predictions. Evaluate your classifier with mean squared error of predicted page load time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51c9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
